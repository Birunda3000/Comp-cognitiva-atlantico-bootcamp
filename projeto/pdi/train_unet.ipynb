{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 13:51:27.082803: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/erick/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-12 13:51:27.082834: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import utils as ut\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Activation, SeparableConv2D, BatchNormalization, UpSampling2D\n",
    "from keras.layers.core import Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "from keras.layers import concatenate as concatenate\n",
    "#from keras.layers.merge import concatenate\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import time\n",
    "inicio = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR1 = '../../../_DATA_/_DATA/ORIGINAL_GOLD/Original'\n",
    "DATADIR2 = '../../../_DATA_/_DATA/ORIGINAL_GOLD/GOLD'\n",
    "TESTDIR = '../../../_DATA_/_DATA/ORIGINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:03<00:00, 11.94it/s]\n",
      "100%|██████████| 45/45 [00:00<00:00, 115.62it/s]\n",
      "100%|██████████| 66/66 [00:01<00:00, 35.64it/s]\n"
     ]
    }
   ],
   "source": [
    "proportion = 1.77777777778 # 16:9\n",
    "k = 33.75#k*32 menor dimensão da imagem (arrendondada)\n",
    "IMG_CHANNELS = 3\n",
    "EPOCHS = 150\n",
    "batch_size = 5\n",
    "save_model = True\n",
    "patience = 20#no improvement patience\n",
    "\n",
    "height = int(np.around((k * proportion), decimals=0).astype('int') * 32) # too low can cause dimension error and must be divisible by 32\n",
    "width = int(k * 32)# too low can cause dimension error and must be divisible by 32\n",
    "IMG_WIDTH_HEIGHT = max(height, width)\n",
    "\n",
    "original,_ = ut.read_imgs(path=DATADIR1, height=height, width=width, resize = True, mode = 'RGB', squared = True, square_color=(255,255,255), show = False, write=False, write_path='original')\n",
    "\n",
    "gold_masks,_ = ut.read_imgs(path=DATADIR2, height=height, width=width, resize = True, mode = 'GRAYSCALE', squared = True, square_color=(0,0,0), show = False, write=False, write_path='gold_masks')\n",
    "\n",
    "test,_ = ut.read_imgs(path=TESTDIR, height=height, width=width, resize = True, mode = 'RGB', squared = True, square_color=(255,255,255), show = False, write=False, write_path='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "X_test = []\n",
    "for image, mask in ut.concatenate2_2_it(original, gold_masks):\n",
    "\n",
    "        X_train.append(image.astype('float32') / 255.0)\n",
    "        #plt.imshow(X_train[-1], cmap='gray')\n",
    "        #plt.show()\n",
    "        \n",
    "        aux = mask.astype('float32') / 255.0    \n",
    "        Y_train.append( np.around(aux, decimals=0).astype('uint8') )\n",
    "        #plt.imshow(Y_train[-1], cmap='gray')\n",
    "        #plt.show()\n",
    "\n",
    "for test_img in test:\n",
    "\n",
    "        X_test.append(test_img.astype('float32') / 255.0)\n",
    "        #plt.imshow(X_test[-1], cmap='gray')\n",
    "        #plt.show()\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(inputs=None, n_filters=32, dropout_prob=0, max_pooling=True):\n",
    "    \n",
    "    conv = Conv2D(n_filters, \n",
    "                  kernel_size = 3,     \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer=tf.keras.initializers.HeNormal())(inputs)\n",
    "    conv = Conv2D(n_filters, \n",
    "                  kernel_size = 3, \n",
    "                  activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_initializer=tf.keras.initializers.HeNormal())(conv)\n",
    "   \n",
    "    if dropout_prob > 0:\n",
    "        conv = Dropout(dropout_prob)(conv)\n",
    "        \n",
    "    if max_pooling:\n",
    "        next_layer = MaxPooling2D(pool_size=(2,2))(conv)\n",
    "  \n",
    "    else:\n",
    "        next_layer = conv\n",
    "        \n",
    "    #conv = BatchNormalization()(conv)\n",
    "    skip_connection = conv\n",
    "    \n",
    "    return next_layer, skip_connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_block(expansive_input, contractive_input, n_filters=32):\n",
    "    \n",
    "    up = Conv2DTranspose(\n",
    "                 n_filters,  \n",
    "                 kernel_size = 3,\n",
    "                 strides=(2,2),\n",
    "                 padding='same')(expansive_input)\n",
    "\n",
    "    merge = concatenate([up, contractive_input], axis=3)# ??????????????????????????---------------\n",
    "\n",
    "    conv = Conv2D(n_filters,  \n",
    "                 kernel_size = 3,   \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer=tf.keras.initializers.HeNormal())(merge)\n",
    "    conv = Conv2D(n_filters,  \n",
    "                 kernel_size = 3,  \n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer=tf.keras.initializers.HeNormal())(conv)\n",
    "    \n",
    "    return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model(input_size=(IMG_WIDTH_HEIGHT, IMG_WIDTH_HEIGHT\n",
    "                           , IMG_CHANNELS), n_filters=32, n_classes=3):\n",
    "    \n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    #contracting path\n",
    "    cblock1 = convolutional_block(inputs, n_filters)\n",
    "    cblock2 = convolutional_block(cblock1[0], 2*n_filters)\n",
    "    cblock3 = convolutional_block(cblock2[0], 4*n_filters)\n",
    "    cblock4 = convolutional_block(cblock3[0], 8*n_filters, dropout_prob=0.2) \n",
    "    cblock5 = convolutional_block(cblock4[0],16*n_filters, dropout_prob=0.2, max_pooling=None)     \n",
    "    \n",
    "    #expanding path\n",
    "    ublock6 = upsampling_block(cblock5[0], cblock4[1],  8 * n_filters)\n",
    "    ublock7 = upsampling_block(ublock6, cblock3[1],  n_filters*4)\n",
    "    ublock8 = upsampling_block(ublock7,cblock2[1] , n_filters*2)\n",
    "    ublock9 = upsampling_block(ublock8,cblock1[1],  n_filters)\n",
    "    conv9 = Conv2D(n_classes,\n",
    "                 1,\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_normal')(ublock9)\n",
    "    \n",
    "    #conv10 = Conv2D(n_classes, kernel_size=1, padding='same', activation = 'softmax')(conv9) \n",
    "    conv10 = Activation('softmax')(conv9)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv10)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = unet_model((IMG_WIDTH_HEIGHT, IMG_WIDTH_HEIGHT, IMG_CHANNELS), n_classes=3)\n",
    "tf.keras.utils.plot_model(unet, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 13:51:40.268065: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/erick/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-06-12 13:51:40.268105: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-12 13:51:40.268142: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lasid30): /proc/driver/nvidia/version does not exist\n",
      "2022-06-12 13:51:40.268444: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "8/8 [==============================] - 758s 92s/step - loss: 0.2034 - accuracy: 0.9257 - val_loss: 0.1167 - val_accuracy: 0.9471\n",
      "Epoch 2/150\n",
      "8/8 [==============================] - 729s 91s/step - loss: 0.0820 - accuracy: 0.9634 - val_loss: 0.1096 - val_accuracy: 0.9471\n",
      "Epoch 3/150\n",
      "8/8 [==============================] - 730s 91s/step - loss: 0.0785 - accuracy: 0.9640 - val_loss: 0.1301 - val_accuracy: 0.9471\n",
      "Epoch 4/150\n",
      "8/8 [==============================] - 718s 90s/step - loss: 0.0843 - accuracy: 0.9634 - val_loss: 0.1060 - val_accuracy: 0.9472\n",
      "Epoch 5/150\n",
      "8/8 [==============================] - 722s 91s/step - loss: 0.0695 - accuracy: 0.9658 - val_loss: 0.0969 - val_accuracy: 0.9494\n",
      "Epoch 6/150\n",
      "8/8 [==============================] - 725s 91s/step - loss: 0.0573 - accuracy: 0.9741 - val_loss: 0.0872 - val_accuracy: 0.9608\n",
      "Epoch 7/150\n",
      "8/8 [==============================] - 728s 91s/step - loss: 0.0411 - accuracy: 0.9836 - val_loss: 0.0356 - val_accuracy: 0.9855\n",
      "Epoch 8/150\n",
      "8/8 [==============================] - 725s 91s/step - loss: 0.0714 - accuracy: 0.9652 - val_loss: 0.1730 - val_accuracy: 0.9548\n",
      "Epoch 9/150\n",
      "8/8 [==============================] - 730s 91s/step - loss: 0.1427 - accuracy: 0.9643 - val_loss: 0.2323 - val_accuracy: 0.9471\n",
      "Epoch 10/150\n",
      "8/8 [==============================] - 718s 90s/step - loss: 0.1137 - accuracy: 0.9634 - val_loss: 0.1183 - val_accuracy: 0.9472\n",
      "Epoch 11/150\n",
      "8/8 [==============================] - 727s 91s/step - loss: 0.0767 - accuracy: 0.9638 - val_loss: 0.1009 - val_accuracy: 0.9473\n",
      "Epoch 12/150\n",
      "8/8 [==============================] - 720s 90s/step - loss: 0.0684 - accuracy: 0.9637 - val_loss: 0.1024 - val_accuracy: 0.9473\n",
      "Epoch 13/150\n",
      "8/8 [==============================] - 721s 91s/step - loss: 0.0644 - accuracy: 0.9647 - val_loss: 0.0944 - val_accuracy: 0.9488\n",
      "Epoch 14/150\n",
      "8/8 [==============================] - 733s 92s/step - loss: 0.0596 - accuracy: 0.9699 - val_loss: 0.0855 - val_accuracy: 0.9568\n",
      "Epoch 15/150\n",
      "8/8 [==============================] - 725s 91s/step - loss: 0.0451 - accuracy: 0.9796 - val_loss: 0.0958 - val_accuracy: 0.9941\n",
      "Epoch 16/150\n",
      "8/8 [==============================] - 734s 91s/step - loss: 0.0685 - accuracy: 0.9790 - val_loss: 0.1117 - val_accuracy: 0.9621\n",
      "Epoch 17/150\n",
      "8/8 [==============================] - 730s 92s/step - loss: 0.0355 - accuracy: 0.9872 - val_loss: 0.0229 - val_accuracy: 0.9944\n",
      "Epoch 18/150\n",
      "8/8 [==============================] - 721s 91s/step - loss: 0.0137 - accuracy: 0.9966 - val_loss: 0.0191 - val_accuracy: 0.9949\n",
      "Epoch 19/150\n",
      "8/8 [==============================] - 718s 90s/step - loss: 0.0180 - accuracy: 0.9938 - val_loss: 0.0283 - val_accuracy: 0.9932\n",
      "Epoch 20/150\n",
      "8/8 [==============================] - 719s 90s/step - loss: 0.0121 - accuracy: 0.9968 - val_loss: 0.0359 - val_accuracy: 0.9912\n",
      "Epoch 21/150\n",
      "8/8 [==============================] - 731s 92s/step - loss: 0.0152 - accuracy: 0.9945 - val_loss: 0.0230 - val_accuracy: 0.9943\n",
      "Epoch 22/150\n",
      "8/8 [==============================] - 729s 92s/step - loss: 0.0086 - accuracy: 0.9981 - val_loss: 0.0218 - val_accuracy: 0.9947\n",
      "Epoch 23/150\n",
      "8/8 [==============================] - 722s 91s/step - loss: 0.0079 - accuracy: 0.9981 - val_loss: 0.0192 - val_accuracy: 0.9956\n",
      "Epoch 24/150\n",
      "8/8 [==============================] - 732s 92s/step - loss: 0.0068 - accuracy: 0.9984 - val_loss: 0.0209 - val_accuracy: 0.9956\n",
      "Epoch 25/150\n",
      "8/8 [==============================] - 730s 92s/step - loss: 0.0065 - accuracy: 0.9984 - val_loss: 0.0290 - val_accuracy: 0.9943\n",
      "Epoch 26/150\n",
      "8/8 [==============================] - 724s 91s/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.0180 - val_accuracy: 0.9959\n",
      "Epoch 27/150\n",
      "8/8 [==============================] - 722s 91s/step - loss: 0.0063 - accuracy: 0.9984 - val_loss: 0.0233 - val_accuracy: 0.9952\n",
      "Epoch 28/150\n",
      "8/8 [==============================] - 720s 90s/step - loss: 0.0057 - accuracy: 0.9986 - val_loss: 0.0190 - val_accuracy: 0.9960\n",
      "Epoch 29/150\n",
      "8/8 [==============================] - 727s 91s/step - loss: 0.0053 - accuracy: 0.9987 - val_loss: 0.0201 - val_accuracy: 0.9958\n",
      "Epoch 30/150\n",
      "8/8 [==============================] - 734s 92s/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0239 - val_accuracy: 0.9952\n",
      "Epoch 31/150\n",
      "8/8 [==============================] - 732s 92s/step - loss: 0.0054 - accuracy: 0.9986 - val_loss: 0.0189 - val_accuracy: 0.9960\n",
      "Epoch 32/150\n",
      "8/8 [==============================] - 729s 92s/step - loss: 0.0057 - accuracy: 0.9987 - val_loss: 0.0166 - val_accuracy: 0.9962\n",
      "Epoch 33/150\n",
      "8/8 [==============================] - 731s 92s/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0226 - val_accuracy: 0.9954\n",
      "Epoch 34/150\n",
      "8/8 [==============================] - 735s 92s/step - loss: 0.0052 - accuracy: 0.9986 - val_loss: 0.0119 - val_accuracy: 0.9967\n",
      "Epoch 35/150\n",
      "8/8 [==============================] - 743s 93s/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0273 - val_accuracy: 0.9950\n",
      "Epoch 36/150\n",
      "8/8 [==============================] - 737s 92s/step - loss: 0.0055 - accuracy: 0.9987 - val_loss: 0.0148 - val_accuracy: 0.9964\n",
      "Epoch 37/150\n",
      "8/8 [==============================] - 731s 91s/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0223 - val_accuracy: 0.9961\n",
      "Epoch 38/150\n",
      "8/8 [==============================] - 733s 92s/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0141 - val_accuracy: 0.9968\n",
      "Epoch 39/150\n",
      "8/8 [==============================] - 723s 91s/step - loss: 0.0040 - accuracy: 0.9990 - val_loss: 0.0193 - val_accuracy: 0.9963\n",
      "Epoch 40/150\n",
      "8/8 [==============================] - 728s 92s/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0119 - val_accuracy: 0.9972\n",
      "Epoch 41/150\n",
      "8/8 [==============================] - 722s 91s/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0143 - val_accuracy: 0.9969\n",
      "Epoch 42/150\n",
      "8/8 [==============================] - 736s 92s/step - loss: 0.0031 - accuracy: 0.9992 - val_loss: 0.0111 - val_accuracy: 0.9972\n",
      "Epoch 43/150\n",
      "8/8 [==============================] - 728s 91s/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 0.0056 - val_accuracy: 0.9977\n",
      "Epoch 44/150\n",
      "8/8 [==============================] - 725s 91s/step - loss: 0.0059 - accuracy: 0.9986 - val_loss: 0.0196 - val_accuracy: 0.9956\n",
      "Epoch 45/150\n",
      "8/8 [==============================] - 729s 91s/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0103 - val_accuracy: 0.9972\n",
      "Epoch 46/150\n",
      "8/8 [==============================] - 730s 92s/step - loss: 0.0030 - accuracy: 0.9992 - val_loss: 0.0114 - val_accuracy: 0.9972\n",
      "Epoch 47/150\n",
      "8/8 [==============================] - 725s 91s/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0119 - val_accuracy: 0.9966\n",
      "Epoch 48/150\n",
      "8/8 [==============================] - 728s 91s/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0089 - val_accuracy: 0.9971\n",
      "Epoch 49/150\n",
      "8/8 [==============================] - 731s 91s/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0112 - val_accuracy: 0.9968\n",
      "Epoch 50/150\n",
      "8/8 [==============================] - 731s 92s/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0115 - val_accuracy: 0.9967\n",
      "Epoch 51/150\n",
      "8/8 [==============================] - 729s 92s/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0043 - val_accuracy: 0.9981\n",
      "Epoch 52/150\n",
      "8/8 [==============================] - 734s 92s/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0098 - val_accuracy: 0.9969\n",
      "Epoch 53/150\n",
      "1/8 [==>...........................] - ETA: 10:31 - loss: 0.0015 - accuracy: 0.9996"
     ]
    }
   ],
   "source": [
    "unet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "earlystopper = EarlyStopping(patience=patience, verbose=1)\n",
    "model_history = unet.fit(X_train, Y_train, validation_split=0.1, batch_size=batch_size, epochs=EPOCHS, callbacks=[earlystopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = unet.predict(X_train)\n",
    "test_predictions = unet.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import try_all_threshold\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.filters import threshold_otsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "\n",
    "    # making subplots\n",
    "    fig, ax = plt.subplots(1, 3)\n",
    "\n",
    "    # set data with subplots and plot  \n",
    "    ax[0].imshow(X_train[i])\n",
    "    ax[0].set_title('Original')\n",
    "    \n",
    "    ax[1].imshow(Y_train[i], cmap='gray')\n",
    "    ax[1].set_title('Gold Mask')\n",
    "    \n",
    "    grays = rgb2gray(train_predictions[i])\n",
    "    thresh = threshold_otsu( grays )    \n",
    "    binary = grays > thresh\n",
    "\n",
    "    ax[2].imshow(binary, cmap='gray')\n",
    "    ax[2].set_title('Predicted Mask')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_predictions)):\n",
    "\n",
    "    # making subplots\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "    # set data with subplots and plot  \n",
    "    ax[0].imshow(X_test[i])\n",
    "    ax[0].set_title('Original')\n",
    "    \n",
    "    grays = rgb2gray(test_predictions[i])\n",
    "    thresh = threshold_otsu( grays )    \n",
    "    binary = grays > thresh\n",
    "\n",
    "    ax[1].imshow(binary, cmap='gray')\n",
    "    ax[1].set_title('Predicted Mask')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_model:\n",
    "    nome_saida = f'models/model-k={k}.h5'\n",
    "    #nome_saida = caminho_modelo+nome+'.h5'\n",
    "    unet.save(nome_saida)\n",
    "    print('Salvo como: ',nome_saida)\n",
    "print('Tempo de execução: ', time.time() - inicio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['Original file name', 'area_em_pixels', 'altura_em_pixels', 'largura_em_pixels','IOU']\n",
    "data = []\n",
    "\n",
    "DATADIR1 = pathlib.Path(DATADIR1)\n",
    "DATADIR1_files = os.listdir(DATADIR1)\n",
    "\n",
    "DATADIR2 = pathlib.Path(DATADIR2)\n",
    "DATADIR2_files = os.listdir(DATADIR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_predictions)):\n",
    "    \n",
    "    grays = rgb2gray(train_predictions[i])\n",
    "    thresh = threshold_otsu( grays )    \n",
    "    binary = grays > thresh\n",
    "\n",
    "\n",
    "    data.append(ut.take_area(X_test[i], DATADIR1_files[i]) + [ut.IOU(Y_train[i], binary)] )\n",
    "\n",
    "df = pd.DataFrame(data=data, columns =colunas)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tempo de execução: ', time.time() - inicio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "039eedbdc3eafb3ff56e892b04b49d9f6f15fe71eac5c31af6e138083d479011"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
