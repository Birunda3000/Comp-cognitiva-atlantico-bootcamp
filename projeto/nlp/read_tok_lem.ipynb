{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15beed38-f4db-4e82-bc86-7b1d08cd120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 15:04:37.825240: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 15:04:37.825271: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35c8a166ee724a04b681d1c6b996a2bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 15:04:43 WARNING: Language pt package default expects mwt, which has been added\n",
      "2022-05-10 15:04:43 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-05-10 15:04:43 INFO: Use device: cpu\n",
      "2022-05-10 15:04:43 INFO: Loading: tokenize\n",
      "2022-05-10 15:04:43 INFO: Loading: mwt\n",
      "2022-05-10 15:04:43 INFO: Loading: lemma\n",
      "2022-05-10 15:04:43 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../_data/cleaned_text-1.txt\n",
      "../_data/cleaned_text-2.txt\n",
      "../_data/cleaned_text-3.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pdfplumber\n",
    "import re\n",
    "import stanza\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.probability import FreqDist#*\n",
    "import matplotlib.pyplot as plt#*\n",
    "\n",
    "def file_check (file_name, label='texto'):# Returns True or False\n",
    "    return label in file_name\n",
    "\n",
    "def read_files(DATADIR):\n",
    "    DATADIR = pathlib.Path(DATADIR)\n",
    "    PDFs = os.listdir(DATADIR)\n",
    "    PDFs.sort()\n",
    "    PDFs = filter(file_check, PDFs)\n",
    "    PDFs = list(PDFs)\n",
    "    texts = []\n",
    "    for pdf in PDFs:\n",
    "        path = os.path.join(DATADIR, pdf)\n",
    "        temp = pdfplumber.open(path)\n",
    "        t = ''\n",
    "        for page in temp.pages:\n",
    "            page = page.extract_text()        \n",
    "            t=t+' '+page\n",
    "        texts.append(t)\n",
    "    return texts\n",
    "\n",
    "def clean_special_characters(texts):\n",
    "    cleaned_texts_list = []\n",
    "    for text in texts:        \n",
    "        text = re.sub(u'-', ' ', text)#palavras com - à\n",
    "        cleaned_text = re.sub(u'[^a-zA-ZàáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇñÑ ]', ' ', text)\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        cleaned_texts_list.append(cleaned_text)\n",
    "    return cleaned_texts_list\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    texts_list = []    \n",
    "    for text in texts:       \n",
    "        text = text.split()       \n",
    "        stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "        stop = set(stopwords)      \n",
    "        text_no_stop = [w.strip() for w in text]       \n",
    "        text_no_stop = [w for w in text if w not in stop]       \n",
    "        text_no_stop = \" \".join(text_no_stop)        \n",
    "        texts_list.append(text_no_stop)    \n",
    "    return texts_list\n",
    "\n",
    "def tokenization_lemmatization(pathModelStanza, texts):\n",
    "    nlp = stanza.Pipeline(lang='pt', processors='tokenize,lemma', model_dir=pathModelStanza)\n",
    "    text_list = []\n",
    "    for text in texts:    \n",
    "        doc = nlp(text)\n",
    "        text_list.append(doc)\n",
    "    return text_list\n",
    "\n",
    "def imprimir_lemantizacao(doc):\n",
    "    print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\n' for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "def write_text(texts, path=''):  \n",
    "    for i in range(len(texts)):\n",
    "        path_w = os.path.join(path,'cleaned_text-'+str(i+1)+'.txt')\n",
    "        print(path_w)#exibe os arquivos criados\n",
    "        with open(path_w, \"w\") as text_file:\n",
    "            [text_file.write(f'{word.lemma} ') for sent in texts[i].sentences for word in sent.words]\n",
    "\n",
    "def pre_processing(path='', output_path='text', model_path='stanza_models'):\n",
    "    texts = read_files(path)\n",
    "    cleaned_text = clean_special_characters(texts)\n",
    "    text_no_stop = remove_stopwords(cleaned_text)\n",
    "    text_lemma = tokenization_lemmatization(model_path, text_no_stop)\n",
    "    write_text(texts=text_lemma, path=output_path)\n",
    "\n",
    "pre_processing(path='../_data', output_path='../_data', model_path='stanza_models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "10047a3256da40d2928043711b4b0810": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "16ca0fcb86ab4265bd95c4337236ff6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "17f1dbfe24344aeea1e3931f4e461183": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_10047a3256da40d2928043711b4b0810",
       "style": "IPY_MODEL_71e1fe7736d144a38f2680f183db045b",
       "value": " 154k/? [00:00&lt;00:00, 4.37MB/s]"
      }
     },
     "24982e8d545a4e28853889bcf94e7fa7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "35c8a166ee724a04b681d1c6b996a2bf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_d749d2b738ac44cdbb9002ae2acb0106",
        "IPY_MODEL_d026ff2659d24863a523283dea04a3f9",
        "IPY_MODEL_17f1dbfe24344aeea1e3931f4e461183"
       ],
       "layout": "IPY_MODEL_bb1cbae5e7524201853b9f6d236904cd"
      }
     },
     "71e1fe7736d144a38f2680f183db045b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bb1cbae5e7524201853b9f6d236904cd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c7f3ece55bc944b5bf87e3e68523b4ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "d026ff2659d24863a523283dea04a3f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_24982e8d545a4e28853889bcf94e7fa7",
       "max": 25983,
       "style": "IPY_MODEL_f104340010a74f2ca7295f971f06e471",
       "value": 25983
      }
     },
     "d749d2b738ac44cdbb9002ae2acb0106": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_16ca0fcb86ab4265bd95c4337236ff6c",
       "style": "IPY_MODEL_c7f3ece55bc944b5bf87e3e68523b4ce",
       "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: "
      }
     },
     "f104340010a74f2ca7295f971f06e471": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
