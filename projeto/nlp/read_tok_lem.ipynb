{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15beed38-f4db-4e82-bc86-7b1d08cd120d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/erick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 12:01:43.141526: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-10 12:01:43.141556: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5a0485d72de48b48d2c12d6692aea57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 12:01:45 WARNING: Language pt package default expects mwt, which has been added\n",
      "2022-05-10 12:01:45 INFO: Loading these models for language: pt (Portuguese):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | bosque  |\n",
      "| mwt       | bosque  |\n",
      "| lemma     | bosque  |\n",
      "=======================\n",
      "\n",
      "2022-05-10 12:01:45 INFO: Use device: cpu\n",
      "2022-05-10 12:01:45 INFO: Loading: tokenize\n",
      "2022-05-10 12:01:45 INFO: Loading: mwt\n",
      "2022-05-10 12:01:45 INFO: Loading: lemma\n",
      "2022-05-10 12:01:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts/cleaned_text-0.txt\n",
      "texts/cleaned_text-1.txt\n",
      "texts/cleaned_text-2.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import pdfplumber\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.probability import FreqDist\n",
    "import matplotlib.pyplot as plt\n",
    "import stanza\n",
    "\n",
    "def file_check (file_name, label='texto'):# Returns True or False\n",
    "    return label in file_name\n",
    "\n",
    "def read_files(DATADIR):\n",
    "    DATADIR = pathlib.Path(DATADIR)\n",
    "    PDFs = os.listdir(DATADIR)\n",
    "    PDFs.sort()\n",
    "    PDFs = filter(file_check, PDFs)\n",
    "    PDFs = list(PDFs)\n",
    "    texts = []\n",
    "    for pdf in PDFs:\n",
    "        path = os.path.join(DATADIR, pdf)\n",
    "        temp = pdfplumber.open(path)\n",
    "        t = ''\n",
    "        for page in temp.pages:\n",
    "            page = page.extract_text()        \n",
    "            t=t+' '+page\n",
    "        texts.append(t)\n",
    "    return texts\n",
    "\n",
    "def clean_special_characters(texts):\n",
    "    cleaned_texts_list = []\n",
    "    for text in texts:        \n",
    "        text = re.sub(u'-', ' ', text)#palavras com -\n",
    "        cleaned_text = re.sub(u'[^a-zA-ZáéíóúÁÉÍÓÚâêîôÂÊÎÔãõÃÕçÇñÑ ]', '', text)\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "        cleaned_texts_list.append(cleaned_text)\n",
    "    return cleaned_texts_list\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    texts_list = []    \n",
    "    for text in texts:       \n",
    "        text = text.split()       \n",
    "        stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "        stop = set(stopwords)      \n",
    "        text_no_stop = [w.strip() for w in text]       \n",
    "        text_no_stop = [w for w in text if w not in stop]       \n",
    "        text_no_stop = \" \".join(text_no_stop)        \n",
    "        texts_list.append(text_no_stop)    \n",
    "    return texts_list\n",
    "\n",
    "def tokenize_lemantize(pathModelStanza, texts):\n",
    "    nlp = stanza.Pipeline(lang='pt', processors='tokenize,lemma', model_dir=pathModelStanza)\n",
    "    text_list = []\n",
    "    for text in texts:    \n",
    "        doc = nlp(text)\n",
    "        text_list.append(doc)\n",
    "    return text_list\n",
    "\n",
    "def imprimir_lemantizacao(doc):\n",
    "    print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\n' for sent in doc.sentences for word in sent.words])\n",
    "\n",
    "def write_text(texts, path=''):  \n",
    "    for i in range(len(texts)):\n",
    "        path_w = os.path.join(path,'cleaned_text-'+str(i)+'.txt')\n",
    "        print(path_w)#exibe os arquivos criados\n",
    "        with open(path_w, \"w\") as text_file:\n",
    "            [text_file.write(f'{word.lemma}\\n') for sent in texts[i].sentences for word in sent.words]\n",
    "\n",
    "def pre_processing(path='', output_path='text', model_path='stanza_models'):\n",
    "    texts = read_files(path)\n",
    "    cleaned_text = clean_special_characters(texts)\n",
    "    text_no_stop = remove_stopwords(cleaned_text)\n",
    "    text_lemma = tokenize_lemantize(model_path, text_no_stop)\n",
    "    write_text(texts=text_lemma, path=output_path)\n",
    "\n",
    "pre_processing(path='docs', output_path='texts', model_path='stanza_models')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "2122c6c3fd604195b7264b0b127bdec7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3042e9ae6b864d278b0838246855ff79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "424a3f67222a4fd7a377333c1450dc31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6f1929c4c0f94e21b08f3da2b1e1d773": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "79a0c52fa4a9424d9a1272a0e6038535": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "84ee9e99230f4ef286964d87398de104": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_424a3f67222a4fd7a377333c1450dc31",
       "max": 25983,
       "style": "IPY_MODEL_79a0c52fa4a9424d9a1272a0e6038535",
       "value": 25983
      }
     },
     "8e6ae7c261164b8e92ae627742877bdf": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e9d2ad1bb24a44b292af896fa746d559",
       "style": "IPY_MODEL_a081c9f77de44163bd85f8a2d3fa59ea",
       "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.0.json: "
      }
     },
     "a081c9f77de44163bd85f8a2d3fa59ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "b00fd30cde8c4985a4bee2f4c3ab7e8d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3042e9ae6b864d278b0838246855ff79",
       "style": "IPY_MODEL_6f1929c4c0f94e21b08f3da2b1e1d773",
       "value": " 154k/? [00:00&lt;00:00, 4.65MB/s]"
      }
     },
     "e9d2ad1bb24a44b292af896fa746d559": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5a0485d72de48b48d2c12d6692aea57": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_8e6ae7c261164b8e92ae627742877bdf",
        "IPY_MODEL_84ee9e99230f4ef286964d87398de104",
        "IPY_MODEL_b00fd30cde8c4985a4bee2f4c3ab7e8d"
       ],
       "layout": "IPY_MODEL_2122c6c3fd604195b7264b0b127bdec7"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
