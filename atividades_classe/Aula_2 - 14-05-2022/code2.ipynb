{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8510948-61b3-47a7-ae7a-89cd63a2d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import WordNetLemmatizer and Counter\n",
    "__\n",
    "__\n",
    "from NLP.src.nlp_utils import get_wiki_article_lower_tokens, get_english_stop_words\n",
    "\n",
    "\n",
    "lower_tokens = get_wiki_article_lower_tokens()\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [_ for t in __ if __]\n",
    "\n",
    "english_stop = get_english_stop_words()\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [__ for t in __ if __ not in __]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = __()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [__(__) for t in __]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = __\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
